{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Classifier Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c4e326afbd2829f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parallel to the implementation of our CNN, we have decided it is a good idea to train some classifiers on the same dataset. We will use the following classifiers:\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- SVM\n",
    "- KNN\n",
    "- AdaBoost\n",
    "- Ensemble of the above classifiers\n",
    "\n",
    "We will analyze the performance of the classifiers we have trained on the test set. We will use the following metrics:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score\n",
    "- ROC AUC\n",
    "- Confusion Matrix"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a90bdb8d295da96d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "617260635f45778e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For logistic regression, we found problem with the default solver, so we used the 'saga' solver as it is better suited for larger datasets.\n",
    "\n",
    "We had an issue with the solver not converging, so we increased the maximum number of iterations to 5000 and after having found best Inverse of Regularization Strength (C) parameter using GridSearchCV, we trained the model with the best C value and looking for other hyperparams.\n",
    "\n",
    "We found that the best hyperparameters for the model were:\n",
    "- C: 200\n",
    "- penalty: l1\n",
    "- solver: saga\n",
    "- max_iter: 5000\n",
    "- tol: 0.0001\n",
    "\n",
    "However, even with these hyperparameters, the model did not perform well on the test set. The accuracy was around 0.57, which is almost the same as random guessing. The precision, recall, and F1 score were also 0.57.\n",
    "\n",
    "\n",
    "## Confusion matrix\n",
    "![img](../classifiers/diagrams/log_reg/cm.png)\n",
    "\n",
    "## ROC curve\n",
    "![img](../classifiers/diagrams/log_reg/roc_curve.png)\n",
    "\n",
    "## Precision-Recall curve\n",
    "![img](../classifiers/diagrams/log_reg/pr_curve.png)\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "Logistic Regression did not perform well on the dataset. The model was not able to learn the patterns in the data and performed almost the same as random guessing.\n",
    "Due to that result, we did not want to include the model in the ensemble model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ad90d8064927750"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a15e04fa9d2dda85"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For Random Forest Classifier, we were really satisfied with achieved results. \n",
    "Again, GridSearchCV was used to find the best hyperparameters for the model.\n",
    "\n",
    "We did so iteratively, starting with selected hyperparameters and adding them on top of each other due to computing limitation of our hardware.\n",
    "\n",
    "We found that the best hyperparameters for the model were:\n",
    "- n_estimators: 300\n",
    "- max_depth: 50\n",
    "- min_samples_split: 2\n",
    "- min_samples_leaf: 1\n",
    "- max_features: sqrt\n",
    "- criterion: entropy\n",
    "- bootstrap: False\n",
    "\n",
    "With these hyperparameters, the model performed well on the test set. The accuracy was around 0.87, which is a good result. The precision, recall, and F1 score were also around 0.87.\n",
    "\n",
    "## Confusion matrix\n",
    "![img](../classifiers/diagrams/random_forest/cm.png)\n",
    "\n",
    "## ROC curve\n",
    "![img](../classifiers/diagrams/random_forest/roc_curve.png)\n",
    "\n",
    "## Precision-Recall curve\n",
    "![img](../classifiers/diagrams/random_forest/pr_curve.png)\n",
    "\n",
    "## Conclusion\n",
    "Random Forest Classifier performed best on the dataset. The model was able to learn the patterns in the data and performed around 0.87 accuracy. We decided to include the model in the ensemble model. Given its performance, it is a good classifier to match with our CNN."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e69c74cd16c77828"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# K-Nearest Neighbors Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53e8d8141ece6690"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For K-Nearest Neighbors Classifier, we concluded that even with fine-tuning the hyperparameters, we could not improve the model's performance. It was bound to the range of 0.69-0.71 accuracy, which is semi-satisfactory result - but as shown previously, you can find a better classifier for this dataset.\n",
    "\n",
    "Used hyperparameters:\n",
    "- n_neighbors: 3\n",
    "- weights: distance\n",
    "\n",
    "Other hyperparameters were set to default as they performed seemingly best.\n",
    "\n",
    "## Confusion matrix\n",
    "![img](../classifiers/diagrams/knn/cm.png)\n",
    "\n",
    "## ROC curve\n",
    "![img](../classifiers/diagrams/knn/roc_curve.png)\n",
    "\n",
    "## Precision-Recall curve\n",
    "![img](../classifiers/diagrams/knn/pr_curve.png)\n",
    "\n",
    "## Conclusion\n",
    "K-Nearest Neighbors Classifier performed mediocre on this dataset. The model was not able to learn the patterns in the data and performed around 0.70 accuracy. However, we decided to include the model in the ensemble model as it performed better than Logistic Regression."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b403652adca5a808"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Support Vector Machine Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "766bac8b41c0fb44"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Support Vector Machine Classifier was also stubborn to fine-tune. We tried to find the best hyperparameters for the model using GridSearchCV, but the model was not able to learn the patterns in the data and performed around 0.70 accuracy.\n",
    "\n",
    "Used hyperparameters:\n",
    "- C: 100\n",
    "- kernel: rbf\n",
    "- gamma: 0.01\n",
    "\n",
    "Other hyperparameters were set to default as they performed seemingly best.\n",
    "\n",
    "## Confusion matrix\n",
    "![img](../classifiers/diagrams/svm/cm.png)\n",
    "\n",
    "## ROC curve\n",
    "![img](../classifiers/diagrams/svm/roc_curve.png)\n",
    "\n",
    "## Precision-Recall curve\n",
    "![img](../classifiers/diagrams/svm/pr_curve.png)\n",
    "\n",
    "## Conclusion\n",
    "Support Vector Machine Classifier performed mediocre on this dataset. The model was not able to learn the patterns in the data and performed around 0.70 accuracy. However, we decided to include the model in the ensemble model as it performed better than Logistic Regression."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56d349242d18ade"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AdaBoost Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3360785b47656492"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For AdaBoost Classifier, we were dissapointed in the results as the classifier achieved a whooping accuracy of 0.49 on the test set!\n",
    "Which is a pretty much equivalent to rolling a dice to predict the class.\n",
    "\n",
    "Used hyperparameters:\n",
    "- n_estimators: 200\n",
    "- learning_rate: 0.5\n",
    "- algorithm: SAMME\n",
    "\n",
    "other parameters were set as default.\n",
    "\n",
    "## Confusion matrix\n",
    "![img](../classifiers/diagrams/adaboost/cm.png)\n",
    "\n",
    "## ROC curve\n",
    "![img](../classifiers/diagrams/adaboost/roc_curve.png)\n",
    "\n",
    "## Precision-Recall curve\n",
    "![img](../classifiers/diagrams/adaboost/pr_curve.png)\n",
    "\n",
    "## Conclusion\n",
    "There is nothing much to say about AdaBoost Classifier for this dataset. Due to the low accuracy, we decided not to include the model in the ensemble model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b8778194567553"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ensemble Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2339877abeb6ac9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the ensemble classifier, we used the following classifiers:\n",
    "- Random Forest\n",
    "- K-Nearest Neighbors\n",
    "- Support Vector Machine\n",
    "\n",
    "We used the VotingClassifier from sklearn to combine the predictions of the classifiers. We used the 'soft' voting strategy, which predicts the class label based on the argmax of the sums of the predicted probabilities.\n",
    "\n",
    "We also used the VotingClassifier with 'hard' voting strategy, which predicts the class label based on the majority vote.\n",
    "\n",
    "However, due to the fact that two out of three classifiers performed poorly, the ensemble classifier also did not perform well on the dataset. The accuracy was around 0.80 and 0.79 for 'soft' and 'hard' voting strategy, respectively.\n",
    "\n",
    "## Confusion matrix\n",
    "![img](../classifiers/diagrams/ensemble/cm.png)\n",
    "\n",
    "## ROC curve\n",
    "![img](../classifiers/diagrams/ensemble/roc_curve.png)\n",
    "\n",
    "## Precision-Recall curve\n",
    "![img](../classifiers/diagrams/ensemble/pr_curve.png)\n",
    "\n",
    "## Conclusion\n",
    "The ensemble classifier did not manage to improve our accuracy. It is needed to get a better classifiers to improve the results and probably give a thought to remove the KNN and SVM from the ensemble model.\n",
    "\n",
    "This shows, that the Random Forest Classifier is miles ahead of other classifiers for this dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1844cbdb860fa9c6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
